{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import os\n",
    "import sys\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from emoji import demojize\n",
    "\n",
    "import random\n",
    "import math\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import string\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split, KFold\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "import transformers\n",
    "from transformers import TFAutoModel, AutoTokenizer, TFAutoModelForSequenceClassification\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "print('Using Tensorflow version:', tf.__version__)\n",
    "print('Using Transformers version:', transformers.__version__)\n",
    "\n",
    "# import warnings\n",
    "# warnings.filterwarnings('ignore')\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\" \n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1512\n",
    "def seed_all(seed=1512):\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "seed_all(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "train_df = pd.read_csv(\"../data/tokenized_data/train_5_folds.csv\")\n",
    "test_df = pd.read_csv(\"../data/tokenized_data/test.csv\")\n",
    "\n",
    "train_df[\"post_message\"] = train_df[\"post_message\"].astype(str)\n",
    "test_df[\"post_message\"] = test_df[\"post_message\"].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roberta = 'vinai/phobert-base' \n",
    "roberta_tokenizer = AutoTokenizer.from_pretrained(roberta)\n",
    "roberta_model = TFAutoModel.from_pretrained(roberta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_len_word = [len(text.split()) for text in train_df.post_message]\n",
    "# test_len_word = [len(text.split()) for text in test_df.post_message]\n",
    "# test_len_char = [len(text) for text in train_df.post_message]\n",
    "# test_len_char = [len(text) for text in test_df.post_message]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def length_plot(lengths):\n",
    "#     plt.figure(figsize=(12,7))\n",
    "#     textstr = f' Mean: {np.mean(lengths):.2f} \\u00B1 {np.std(lengths):.2f} \\n Max: {np.max(lengths)}'\n",
    "#     props = dict(boxstyle='round', facecolor='wheat', alpha=0.5)\n",
    "\n",
    "#     plt.text(0, 0, textstr, fontsize=14,\n",
    "#             verticalalignment='top', bbox=props)\n",
    "#     sns.countplot(lengths)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# length_plot(train_len_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# length_plot(test_len_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 256\n",
    "BATCH_SIZE = 48"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regular_encode(texts, maxlen=MAX_LEN):\n",
    "\n",
    "    roberta_enc_di = roberta_tokenizer.batch_encode_plus(\n",
    "        texts,\n",
    "        return_token_type_ids=True,\n",
    "        pad_to_max_length=True,\n",
    "        max_length=maxlen,\n",
    "        truncation=True,\n",
    "    )\n",
    "    \n",
    "    roberta_enc = (\n",
    "        np.array(roberta_enc_di[\"input_ids\"]),\n",
    "        np.array(roberta_enc_di[\"attention_mask\"]),\n",
    "        np.array(roberta_enc_di[\"token_type_ids\"]),\n",
    "    )\n",
    "    return roberta_enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTO = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "def data_generator(train_df, val_df):\n",
    "\n",
    "    X_train = regular_encode(train_df[\"post_message\"].values, maxlen=MAX_LEN)\n",
    "    # y_train = tf.keras.utils.to_categorical(train_df['Label'].values, num_classes=2)\n",
    "    y_train = train_df[\"label\"].values\n",
    "    X_val = regular_encode(val_df[\"post_message\"].values, maxlen=MAX_LEN)\n",
    "    # y_val = tf.keras.utils.to_categorical(val_df['Label'].values, num_classes=2)\n",
    "    y_val = val_df[\"label\"].values\n",
    "\n",
    "    train_dataset = (\n",
    "        tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "        .repeat()\n",
    "        .shuffle(1024)\n",
    "        .batch(BATCH_SIZE)\n",
    "        .prefetch(AUTO)\n",
    "    )\n",
    "\n",
    "    valid_dataset = (\n",
    "        tf.data.Dataset.from_tensor_slices((X_val, y_val))\n",
    "        .batch(BATCH_SIZE)\n",
    "        .cache()\n",
    "        .prefetch(AUTO)\n",
    "    )\n",
    "\n",
    "    return train_dataset, valid_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(bert_model_name_or_path=\"vinai/phobert-base\", max_len=384, n_hiddens=4):\n",
    "    bert_model = TFAutoModel.from_pretrained(bert_model_name_or_path)\n",
    "\n",
    "    bert_input_word_ids = tf.keras.layers.Input(\n",
    "        shape=(max_len,), dtype=tf.int32, name=\"bert_input_id\"\n",
    "    )\n",
    "    bert_attention_mask = tf.keras.layers.Input(\n",
    "        shape=(max_len,), dtype=tf.int32, name=\"bert_attention_mask\"\n",
    "    )\n",
    "    bert_token_type_ids = tf.keras.layers.Input(\n",
    "        shape=(max_len,), dtype=tf.int32, name=\"bert_token_type_ids\"\n",
    "    )\n",
    "\n",
    "    bert_sequence_output = bert_model(\n",
    "        bert_input_word_ids,\n",
    "        attention_mask=bert_attention_mask,\n",
    "        token_type_ids=bert_token_type_ids,\n",
    "        output_hidden_states=True,\n",
    "        output_attentions=True,\n",
    "        \n",
    "    )\n",
    "\n",
    "    # print(len(bert_sequence_output)) # 4\n",
    "\n",
    "    # print(bert_sequence_output[0].shape) # (None, max_len, 768)\n",
    "\n",
    "    # print(bert_sequence_output[1].shape) # (None, 768)\n",
    "    # print(len(bert_sequence_output[2])) # 13\n",
    "    # print(bert_sequence_output[2][0].shape) # (None, max_len, 768)\n",
    "    # print(len(bert_sequence_output[3])) # 12\n",
    "    # print(bert_sequence_output[3][0].shape) # (None, 12, None, max_len)\n",
    "\n",
    "    # TODO: get bert embedding\n",
    "\n",
    "    if n_hiddens == -1:  # get [CLS] token embedding only\n",
    "        # print(\"Get pooler output of shape (batch_size, hidden_size)\")\n",
    "        bert_sequence_output = bert_sequence_output[0][:, 0, :]\n",
    "    else:  # concatenate n_hiddens final layer\n",
    "        # print(f\"Concatenate {n_hiddens} hidden_states of shape (batch_size, hidden_size)\")\n",
    "        bert_sequence_output = tf.concat(\n",
    "            [bert_sequence_output[2][-i] for i in range(n_hiddens)], axis=-1)\n",
    "\n",
    "    # print(\"bert_sequence_output shape\", bert_sequence_output.shape)\n",
    "\n",
    "    \n",
    "    \n",
    "    out = tf.keras.layers.Flatten()(bert_sequence_output)\n",
    "    out = tf.keras.layers.Dense(1, activation=\"sigmoid\")(out)\n",
    "\n",
    "    model = tf.keras.models.Model(\n",
    "        inputs=[\n",
    "            bert_input_word_ids,\n",
    "            bert_attention_mask,\n",
    "            bert_token_type_ids,  # bert input\n",
    "        ],\n",
    "        outputs=out,\n",
    "    )\n",
    "    model.compile(\n",
    "        tf.keras.optimizers.Adam(lr=5e-5),\n",
    "        loss=\"binary_crossentropy\",\n",
    "        metrics=[tf.keras.metrics.AUC()],\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "model = build_model(max_len=MAX_LEN)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp = 'roberta-baseline' \n",
    "n_splits = 5\n",
    "n_epochs = 5\n",
    "\n",
    "DISPLAY=1 # USE display=1 FOR INTERACTIVE\n",
    "exp = f'phobert_{MAX_LEN}_len'\n",
    "\n",
    "output_dir = f'../outputs/{exp}_models'\n",
    "os.makedirs(output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scheduler(epoch):\n",
    "    return 3e-5*0.2**epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strategy = tf.distribute.MirroredStrategy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # for fold, (idxT, idxV) in enumerate(kf.split(train_df)):\n",
    "# for fold in sorted(train_df[\"fold\"].unique()):\n",
    "#     print('*'*100)\n",
    "#     print(f'FOLD: {fold+1}/{n_splits}')\n",
    "#     if fold<4:\n",
    "#         continue\n",
    "#     print('*'*100)\n",
    "#     print(f'FOLD: {fold+1}/{n_splits}')\n",
    "    \n",
    "#     K.clear_session()\n",
    "#     with strategy.scope():\n",
    "#         model = build_model(max_len=MAX_LEN)\n",
    "        \n",
    "#     reduce_lr = tf.keras.callbacks.LearningRateScheduler(scheduler)\n",
    "\n",
    "#     model_dir = os.path.join(output_dir, f'Fold_{fold+1}.h5')\n",
    "\n",
    "#     sv = tf.keras.callbacks.ModelCheckpoint(model_dir, \n",
    "#                                             monitor='val_auc', \n",
    "#                                             verbose=1, \n",
    "#                                             save_best_only=True,\n",
    "#                                             save_weights_only=True, \n",
    "#                                             mode='max', \n",
    "#                                             save_freq='epoch')\n",
    "    \n",
    "#     train_df_ = train_df[train_df[\"fold\"]!=fold]\n",
    "#     val_df_ = train_df[train_df[\"fold\"]==fold]\n",
    "#     train_dataset, valid_dataset = data_generator(train_df_, val_df_)\n",
    "    \n",
    "#     n_steps = train_df_.shape[0] // BATCH_SIZE\n",
    "#     train_history = model.fit(\n",
    "#         train_dataset,\n",
    "#         steps_per_epoch=n_steps,\n",
    "        \n",
    "#         callbacks=[sv, \n",
    "#             reduce_lr,\n",
    "#             # tb\n",
    "#             ],\n",
    "#         validation_data=valid_dataset,\n",
    "#         epochs=n_epochs\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.predict(valid_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_test = regular_encode(test_df['post_message'].values, maxlen=MAX_LEN)\n",
    "# y_test = np.zeros((len(test_df),1))\n",
    "# test_dataset = (\n",
    "#     tf.data.Dataset\n",
    "#     .from_tensor_slices((X_test,y_test))\n",
    "#     .batch(BATCH_SIZE)\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = build_model(max_len=MAX_LEN)\n",
    "# preds = []\n",
    "# for i, file_name in enumerate(os.listdir(output_dir)):\n",
    "#     print('_'*80)\n",
    "    \n",
    "#     K.clear_session()\n",
    "#     model_path = os.path.join(output_dir, file_name)\n",
    "    \n",
    "#     print(f'Inferencing with model from: {model_path}')\n",
    "#     model.load_weights(model_path)\n",
    "\n",
    "#     pred = model.predict(test_dataset,\n",
    "#                          batch_size=128,\n",
    "#                          verbose=DISPLAY)\n",
    "#     # print(pred[])\n",
    "#     preds.append(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preds = np.mean(preds, axis=0)\n",
    "# test_df[\"prediction\"] = preds\n",
    "# test_df[\"prediction\"].to_csv(f\"{exp}.csv\", header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter(train_df[\"num_like_post\"])[\"unknown\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(train_df[\"num_like_post\"].isna())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter(train_df[\"num_share_post\"])[\"unknown\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(train_df[\"num_share_post\"].isna())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter(train_df[\"num_comment_post\"])[\"unknown\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(train_df[\"num_comment_post\"].isna())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_dict = {\"num_like_post\":-2,\"num_comment_post\":-2, \"num_share_post\":-2}\n",
    "unknown_dict = {\"num_like_post\":-2,\"num_comment_post\":-2, \"num_share_post\":-2}\n",
    "\n",
    "def extract_num(text):\n",
    "    return re.findall(\"\\d+\", text)[0]\n",
    "def vlsp_impute(text, field):\n",
    "    if type(text)!=str:\n",
    "        if math.isnan(text):\n",
    "            return nan_dict[field]\n",
    "    elif text==\"unknown\":        \n",
    "        return unknown_dict[field]\n",
    "    else:\n",
    "        try:\n",
    "            return int(extract_num(text))\n",
    "        except:\n",
    "            print(text)\n",
    "            return nan_dict[field]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for field in tqdm([\"num_like_post\", \"num_comment_post\", \"num_share_post\"]):\n",
    "#     with open(f\"train_{field}.txt\", \"w\") as f:\n",
    "#         f.write(\"\\n\".join([str(x) for x in train_df[field].unique()]))\n",
    "#     with open(f\"test_{field}.txt\", \"w\") as f:\n",
    "#         f.write(\"\\n\".join([str(x) for x in test_df[field].unique()]))    \n",
    "    train_df[field] =  train_df[field].apply(lambda text: vlsp_impute(text, field)).astype(int)\n",
    "    test_df[field] =  test_df[field].apply(lambda text: vlsp_impute(text, field)).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regular_encode(df, max_len=MAX_LEN):\n",
    "    \n",
    "    roberta_enc_di = roberta_tokenizer.batch_encode_plus(\n",
    "        df[\"post_message\"].values,\n",
    "        return_token_type_ids=True,\n",
    "        pad_to_max_length=True,\n",
    "        max_length=max_len,\n",
    "        truncation=True,\n",
    "    )\n",
    "    \n",
    "    num_like_post = (df[\"num_like_post\"]/1e6).values\n",
    "    num_comment_post = (df[\"num_comment_post\"]/1e4).values\n",
    "    num_share_post = (df[\"num_share_post\"]/1e4).values\n",
    "    \n",
    "    roberta_enc = (\n",
    "        np.array(roberta_enc_di[\"input_ids\"]),\n",
    "        np.array(roberta_enc_di[\"attention_mask\"]),\n",
    "        np.array(roberta_enc_di[\"token_type_ids\"]),\n",
    "        num_like_post,\n",
    "        num_comment_post,\n",
    "        num_share_post,\n",
    "#         np.concatenate([num_like_post, num_comment_post, num_share_post], axis=0)\n",
    "    )\n",
    "    \n",
    "    \n",
    "    return roberta_enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTO = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "def data_generator(train_df, val_df):\n",
    "\n",
    "    X_train = regular_encode(train_df, max_len=MAX_LEN)\n",
    "    # y_train = tf.keras.utils.to_categorical(train_df['Label'].values, num_classes=2)\n",
    "    y_train = train_df[\"label\"].values\n",
    "    X_val = regular_encode(val_df, max_len=MAX_LEN)\n",
    "    # y_val = tf.keras.utils.to_categorical(val_df['Label'].values, num_classes=2)\n",
    "    y_val = val_df[\"label\"].values\n",
    "\n",
    "    train_dataset = (\n",
    "        tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "        .repeat()\n",
    "        .shuffle(1024)\n",
    "        .batch(BATCH_SIZE)\n",
    "        .prefetch(AUTO)\n",
    "    )\n",
    "\n",
    "    valid_dataset = (\n",
    "        tf.data.Dataset.from_tensor_slices((X_val, y_val))\n",
    "        .batch(BATCH_SIZE)\n",
    "        .cache()\n",
    "        .prefetch(AUTO)\n",
    "    )\n",
    "\n",
    "    return train_dataset, valid_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(bert_model_name_or_path=\"vinai/phobert-base\", max_len=384, n_hiddens=-1):\n",
    "    bert_model = TFAutoModel.from_pretrained(bert_model_name_or_path)\n",
    "\n",
    "    bert_input_word_ids = tf.keras.layers.Input(\n",
    "        shape=(max_len,), dtype=tf.int32, name=\"bert_input_id\"\n",
    "    )\n",
    "    bert_attention_mask = tf.keras.layers.Input(\n",
    "        shape=(max_len,), dtype=tf.int32, name=\"bert_attention_mask\"\n",
    "    )\n",
    "    bert_token_type_ids = tf.keras.layers.Input(\n",
    "        shape=(max_len,), dtype=tf.int32, name=\"bert_token_type_ids\"\n",
    "    )\n",
    "\n",
    "    bert_sequence_output = bert_model(\n",
    "        bert_input_word_ids,\n",
    "        attention_mask=bert_attention_mask,\n",
    "        token_type_ids=bert_token_type_ids,\n",
    "        output_hidden_states=True,\n",
    "        output_attentions=True,\n",
    "        \n",
    "    )\n",
    "\n",
    "    # print(len(bert_sequence_output)) # 4\n",
    "\n",
    "    # print(bert_sequence_output[0].shape) # (None, max_len, 768)\n",
    "\n",
    "    # print(bert_sequence_output[1].shape) # (None, 768)\n",
    "    # print(len(bert_sequence_output[2])) # 13\n",
    "    # print(bert_sequence_output[2][0].shape) # (None, max_len, 768)\n",
    "    # print(len(bert_sequence_output[3])) # 12\n",
    "    # print(bert_sequence_output[3][0].shape) # (None, 12, None, max_len)\n",
    "\n",
    "    # TODO: get bert embedding\n",
    "\n",
    "    if n_hiddens == -1:  # get [CLS] token embedding only\n",
    "        # print(\"Get pooler output of shape (batch_size, hidden_size)\")\n",
    "        bert_sequence_output = bert_sequence_output[0][:, 0, :]\n",
    "#         bert_sequence_output = bert_sequence_output[1]\n",
    "    else:  # concatenate n_hiddens final layer\n",
    "        # print(f\"Concatenate {n_hiddens} hidden_states of shape (batch_size, hidden_size)\")\n",
    "        bert_sequence_output = tf.concat(\n",
    "            [bert_sequence_output[2][-i] for i in range(n_hiddens)], axis=-1)\n",
    "        bert_sequence_output = bert_sequence_output[:, 0, :]\n",
    "\n",
    "    # print(\"bert_sequence_output shape\", bert_sequence_output.shape)\n",
    "\n",
    "#     bert_output = tf.keras.layers.Flatten()(bert_sequence_output)\n",
    "    bert_output = tf.keras.layers.Dense(16, activation=\"relu\")(bert_sequence_output)\n",
    "#     print(bert_output.shape)\n",
    "    \n",
    "\n",
    "    num_like_post = tf.keras.layers.Input(shape=(1,), dtype=tf.float32, name=\"num_like_post\")\n",
    "    num_comment_post = tf.keras.layers.Input(shape=(1,), dtype=tf.float32, name=\"num_comment_post\")\n",
    "    num_share_post = tf.keras.layers.Input(shape=(1,), dtype=tf.float32, name=\"num_share_post\")\n",
    "    \n",
    "    aulixiary_info = tf.keras.layers.Concatenate()([num_like_post, num_comment_post, num_share_post])\n",
    "#     aulixiary_output  = tf.keras.layers.GaussianNoise(0.2)(aulixiary_info)\n",
    "    \n",
    "    out = tf.keras.layers.Concatenate()([bert_output, aulixiary_info]) \n",
    "#     print(out.shape)\n",
    "    out = tf.keras.layers.Dense(1, activation=\"sigmoid\")(out)\n",
    "\n",
    "    model = tf.keras.models.Model(\n",
    "        inputs=[\n",
    "            bert_input_word_ids,\n",
    "            bert_attention_mask,\n",
    "            bert_token_type_ids,  # bert input\n",
    "            num_like_post,\n",
    "            num_comment_post,\n",
    "            num_share_post,\n",
    "        ],\n",
    "        outputs=out,\n",
    "    )\n",
    "    model.compile(\n",
    "        tf.keras.optimizers.Adam(lr=5e-5),\n",
    "        loss=\"binary_crossentropy\",\n",
    "        metrics=[tf.keras.metrics.AUC()],\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "model = build_model(max_len=MAX_LEN)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_splits = 5\n",
    "n_epochs = 5\n",
    "\n",
    "DISPLAY=1 # USE display=1 FOR INTERACTIVE\n",
    "exp = f'phobert+auxiliary_{MAX_LEN}_len'\n",
    "\n",
    "output_dir = f'../outputs/{exp}_models'\n",
    "os.makedirs(output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scheduler(epoch):\n",
    "    return 3e-5*0.2**epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strategy = tf.distribute.MirroredStrategy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for fold, (idxT, idxV) in enumerate(kf.split(train_df)):\n",
    "for fold in sorted(train_df[\"fold\"].unique()):\n",
    "    print('*'*100)\n",
    "    print(f'FOLD: {fold+1}/{n_splits}')\n",
    "    \n",
    "    K.clear_session()\n",
    "    with strategy.scope():\n",
    "        model = build_model(max_len=MAX_LEN)\n",
    "        \n",
    "    reduce_lr = tf.keras.callbacks.LearningRateScheduler(scheduler)\n",
    "\n",
    "    model_dir = os.path.join(output_dir, f'Fold_{fold+1}.h5')\n",
    "\n",
    "    sv = tf.keras.callbacks.ModelCheckpoint(model_dir, \n",
    "                                            monitor='val_auc', \n",
    "                                            verbose=1, \n",
    "                                            save_best_only=True,\n",
    "                                            save_weights_only=True, \n",
    "                                            mode='max', \n",
    "                                            save_freq='epoch')\n",
    "    \n",
    "    train_df_ = train_df[train_df[\"fold\"]!=fold]\n",
    "    val_df_ = train_df[train_df[\"fold\"]==fold]\n",
    "    train_dataset, valid_dataset = data_generator(train_df_, val_df_)\n",
    "    \n",
    "    n_steps = train_df_.shape[0] // BATCH_SIZE\n",
    "    train_history = model.fit(\n",
    "        train_dataset,\n",
    "        steps_per_epoch=n_steps,\n",
    "        \n",
    "        callbacks=[sv, \n",
    "            reduce_lr,\n",
    "            # tb\n",
    "            ],\n",
    "        validation_data=valid_dataset,\n",
    "        epochs=n_epochs\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = regular_encode(test_df, max_len=MAX_LEN)\n",
    "y_test = np.zeros((len(test_df),1))\n",
    "test_dataset = (\n",
    "    tf.data.Dataset\n",
    "    .from_tensor_slices((X_test,y_test))\n",
    "    .batch(BATCH_SIZE)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(max_len=MAX_LEN)\n",
    "preds = []\n",
    "for i, file_name in enumerate(os.listdir(output_dir)):\n",
    "    print('_'*80)\n",
    "    \n",
    "    K.clear_session()\n",
    "    model_path = os.path.join(output_dir, file_name)\n",
    "    \n",
    "    print(f'Inferencing with model from: {model_path}')\n",
    "    model.load_weights(model_path)\n",
    "\n",
    "    pred = model.predict(test_dataset,\n",
    "                         batch_size=128,\n",
    "                         verbose=DISPLAY)\n",
    "    # print(pred[])\n",
    "    preds.append(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = np.mean(preds, axis=0)\n",
    "test_df[\"prediction\"] = preds\n",
    "test_df[\"prediction\"].to_csv(f\"{exp}.csv\", header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
