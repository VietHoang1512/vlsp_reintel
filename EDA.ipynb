{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import os\n",
    "import sys\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from emoji import demojize\n",
    "\n",
    "import random\n",
    "import math\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import string\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split, KFold\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "import transformers\n",
    "from transformers import TFAutoModel, AutoTokenizer, TFAutoModelForSequenceClassification\n",
    "\n",
    "# from fairseq.data.encoders.fastbpe import fastBPE\n",
    "# from fairseq.data import Dictionary\n",
    "# import argparse\n",
    "\n",
    "# Load rdrsegmenter from VnCoreNLP\n",
    "# from vncorenlp import VnCoreNLP\n",
    "\n",
    "print('Using Tensorflow version:', tf.__version__)\n",
    "print('Using Transformers version:', transformers.__version__)\n",
    "\n",
    "# import warnings\n",
    "# warnings.filterwarnings('ignore')\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\" \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1512\n",
    "def seed_all(seed=1512):\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "seed_all(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '../data/tokenize_data'\n",
    "for fn in os.listdir(data_dir):\n",
    "    print(os.path.join(data_dir, fn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"../data/tokenize_data/train.csv\")\n",
    "test_df = pd.read_csv(\"../data/tokenize_data/test.csv\")\n",
    "print(len(train_df), len(test_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label_map = {\"UNINFORMATIVE\": 0, \"INFORMATIVE\": 1}\n",
    "\n",
    "\n",
    "# def get_labels(file_in, gold_indices=None):\n",
    "#     \"\"\"\n",
    "#     Read the labels from file\n",
    "#     :param file_in: path to the file\n",
    "#     :param gold_indices: set of gold indices\n",
    "#     :return: list of labels\n",
    "#     \"\"\"\n",
    "#     labels = []\n",
    "#     count = 0\n",
    "#     with open(file_in) as reader:\n",
    "#         for line in reader:\n",
    "#             line = line.strip()\n",
    "#             if line == \"Id\\tText\\tLabel\":\n",
    "#                 continue\n",
    "#             count += 1\n",
    "#             if gold_indices is not None and count not in gold_indices:\n",
    "#                 continue\n",
    "\n",
    "#             label = line.split()[-1].upper()\n",
    "#             if len(label) == 0:\n",
    "#                 continue\n",
    "\n",
    "#             if label in label_map:\n",
    "#                 labels.append(label_map[label])\n",
    "#             else:\n",
    "#                 print(\"Error occurs at line {}. \"\n",
    "#                       \"{} is not a label (only support UNINFORMATIVE and INFORMATIVE). \"\n",
    "#                       \"Process terminated.\".format(count + 1, label))\n",
    "#                 sys.exit()\n",
    "\n",
    "\n",
    "#     return labels\n",
    "\n",
    "\n",
    "# def calculate_scores(pred_labels, true_labels, pos_label=label_map[\"INFORMATIVE\"]):\n",
    "#     \"\"\"\n",
    "#     Calculate the precision, recall, f1 and accuracy scores for the predictions\n",
    "#     :param pred_labels: prediction labels\n",
    "#     :param true_labels: ground truth labels\n",
    "#     :param pos_label: INFORMATIVE label\n",
    "#     :return: precision, recall, f1 and accuracy scores\n",
    "#     \"\"\"\n",
    "#     assert len(pred_labels) == len(true_labels)\n",
    "\n",
    "#     tp = 0  # true positive\n",
    "#     fn = 0  # false negative\n",
    "#     fp = 0  # false positive\n",
    "#     n_correct = 0\n",
    "#     for i in range(len(pred_labels)):\n",
    "#         if true_labels[i] == pred_labels[i]:\n",
    "#             n_correct += 1\n",
    "#             if pred_labels[i] == pos_label:\n",
    "#                 tp += 1\n",
    "#         else:\n",
    "#             if pred_labels[i] == pos_label:\n",
    "#                 fp += 1\n",
    "#             else:\n",
    "#                 fn += 1\n",
    "#     # Precision score\n",
    "#     precision = 0.0\n",
    "#     if tp + fp > 0:\n",
    "#         precision = tp * 1.0 / (tp + fp)\n",
    "\n",
    "#     # Recall score\n",
    "#     recall = 0.0\n",
    "#     if tp + fn > 0:\n",
    "#         recall = tp * 1.0 / (tp + fn)\n",
    "\n",
    "#     # F1 score\n",
    "#     f1 = 0.0\n",
    "#     if recall + precision > 0:\n",
    "#         f1 = 2 * precision * recall / (precision + recall)\n",
    "\n",
    "#     # Accuracy score\n",
    "#     accuracy = 0.0\n",
    "#     if len(true_labels) > 0:\n",
    "#         accuracy = n_correct * 1.0 / len(true_labels)\n",
    "#     return precision, recall, f1, accuracy\n",
    "\n",
    "\n",
    "# def evaluate(pred_label_file, true_label_file, gold_indices_file=None):\n",
    "#     gold_indices = get_gold_indices(gold_indices_file)\n",
    "#     pred_labels = get_labels(pred_label_file, gold_indices)\n",
    "#     true_labels = get_labels(true_label_file)\n",
    "#     return calculate_scores(pred_labels, true_labels)\n",
    "\n",
    "\n",
    "# def get_gold_indices(gold_indices_file):\n",
    "#     if gold_indices_file is None:\n",
    "#         return None\n",
    "\n",
    "#     gold_indices = set()\n",
    "#     with open(gold_indices_file, \"r\") as reader:\n",
    "#         for line in reader:\n",
    "#             line = line.strip()\n",
    "#             if len(line) > 0:\n",
    "#                 gold_indices.add(int(line))\n",
    "#     return gold_indices\n",
    "\n",
    "\n",
    "# def score(input_dir, output_dir):\n",
    "#     # unzipped submission data is always in the 'res' subdirectory\n",
    "#     submission_dir = os.path.join(input_dir, 'res')\n",
    "#     submission_file = []\n",
    "#     for el in os.listdir(submission_dir):\n",
    "#         if el.startswith('predictions'):\n",
    "#             submission_file.append(el)\n",
    "#     if not len(submission_file) == 1:\n",
    "#         print(\n",
    "#             \"Warning: the submission folder should only contain 1 file (i.e., 'predictions.txt'). Process terminated.\")\n",
    "#         sys.exit()\n",
    "#     submission_file_name = submission_file[0]\n",
    "#     submission_path = os.path.join(submission_dir, submission_file_name)\n",
    "#     ground_truth_file = os.path.join(input_dir, 'ref', 'groundtruth_data.txt')\n",
    "#     gold_indices_file = os.path.join(input_dir, 'ref', 'gold_indices.txt')\n",
    "#     precision, recall, f1, accuracy = evaluate(submission_path, ground_truth_file, gold_indices_file)\n",
    "#     with open(os.path.join(output_dir, 'scores.txt'), 'w') as output_file:\n",
    "#         output_file.write(\"F1-score:{}\\nPrecision:{}\\nRecall:{}\\nAccuracy:{}\\n\".format(f1, precision, recall, accuracy))\n",
    "\n",
    "\n",
    "# def main():\n",
    "#     [_, input_dir, output_dir] = sys.argv\n",
    "#     score(input_dir, output_dir)\n",
    "\n",
    "\n",
    "# # if __name__ == \"__main__\":\n",
    "# #     # main()\n",
    "# #     [_, pred_label_file, true_label_file] = sys.argv\n",
    "# #     precision, recall, f1, accuracy = evaluate(pred_label_file, true_label_file)\n",
    "# #     print(\"F1-score: {}\\nPrecision: {}\\nRecall: {}\\nAccuracy: {}\\n\".format(f1, precision, recall, accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roberta = 'vinai/phobert-base' \n",
    "roberta_tokenizer = AutoTokenizer.from_pretrained(roberta)\n",
    "roberta_model = TFAutoModel.from_pretrained(roberta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_len_word = [len(text.split()) for text in train_df.post_message]\n",
    "test_len_word = [len(text.split()) for text in test_df.post_message]\n",
    "test_len_char = [len(text) for text in train_df.post_message]\n",
    "test_len_char = [len(text) for text in test_df.post_message]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def length_plot(lengths):\n",
    "    plt.figure(figsize=(12,7))\n",
    "    textstr = f' Mean: {np.mean(lengths):.2f} \\u00B1 {np.std(lengths):.2f} \\n Max: {np.max(lengths)}'\n",
    "    props = dict(boxstyle='round', facecolor='wheat', alpha=0.5)\n",
    "\n",
    "    plt.text(0, 0, textstr, fontsize=14,\n",
    "            verticalalignment='top', bbox=props)\n",
    "    sns.countplot(lengths)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# length_plot(train_len_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# length_plot(test_len_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 184\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regular_encode(texts, maxlen=MAX_LEN):\n",
    "\n",
    "    roberta_enc_di = roberta_tokenizer.batch_encode_plus(\n",
    "        texts,\n",
    "        return_token_type_ids=True,\n",
    "        pad_to_max_length=True,\n",
    "        max_length=maxlen,\n",
    "        truncation=True,\n",
    "    )\n",
    "    \n",
    "    roberta_enc = (\n",
    "        np.array(roberta_enc_di[\"input_ids\"]),\n",
    "        np.array(roberta_enc_di[\"attention_mask\"]),\n",
    "        np.array(roberta_enc_di[\"token_type_ids\"]),\n",
    "    )\n",
    "    return roberta_enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTO = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "def data_generator(train_df, val_df):\n",
    "\n",
    "    X_train = regular_encode(train_df[\"post_message\"].values, maxlen=MAX_LEN)\n",
    "    # y_train = tf.keras.utils.to_categorical(train_df['Label'].values, num_classes=2)\n",
    "    y_train = train_df[\"label\"].values\n",
    "    X_val = regular_encode(val_df[\"post_message\"].values, maxlen=MAX_LEN)\n",
    "    # y_val = tf.keras.utils.to_categorical(val_df['Label'].values, num_classes=2)\n",
    "    y_val = val_df[\"label\"].values\n",
    "\n",
    "    train_dataset = (\n",
    "        tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "        .repeat()\n",
    "        .shuffle(1024)\n",
    "        .batch(BATCH_SIZE)\n",
    "        .prefetch(AUTO)\n",
    "    )\n",
    "\n",
    "    valid_dataset = (\n",
    "        tf.data.Dataset.from_tensor_slices((X_val, y_val))\n",
    "        .batch(BATCH_SIZE)\n",
    "        .cache()\n",
    "        .prefetch(AUTO)\n",
    "    )\n",
    "\n",
    "    return train_dataset, valid_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def f1_m(y_true, y_pred):\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(max_len=MAX_LEN):\n",
    "    roberta_model = TFAutoModel.from_pretrained(roberta)\n",
    "\n",
    "    roberta_input_word_ids = tf.keras.layers.Input(\n",
    "        shape=(max_len,), dtype=tf.int32, name=\"roberta_input_id\"\n",
    "    )\n",
    "    roberta_attention_mask = tf.keras.layers.Input(\n",
    "        shape=(max_len,), dtype=tf.int32, name=\"roberta_attention_mask\"\n",
    "    )\n",
    "    roberta_token_type_ids = tf.keras.layers.Input(\n",
    "        shape=(max_len,), dtype=tf.int32, name=\"roberta_token_type_ids\"\n",
    "    )\n",
    "\n",
    "    roberta_sequence_output = roberta_model(\n",
    "        roberta_input_word_ids,\n",
    "        attention_mask=roberta_attention_mask,\n",
    "        token_type_ids=roberta_token_type_ids,\n",
    "        output_hidden_states=True,\n",
    "        output_attentions=True,\n",
    "    )[0]\n",
    "    # n_hiddens = 4\n",
    "    # print(len(roberta_sequence_output))\n",
    "\n",
    "    # print(roberta_sequence_output[0].shape)\n",
    "    # print(roberta_sequence_output[1].shape)\n",
    "\n",
    "    # roberta_sequence_output = tf.concat([roberta_sequence_output[2][-i] for i in range(n_hiddens)] , axis=-1)\n",
    "\n",
    "\n",
    "    roberta_sequence_output = roberta_sequence_output[:, 0, :]\n",
    "    sequence_output = tf.concat(\n",
    "        [ \n",
    "            roberta_sequence_output,\n",
    "\n",
    "        ],\n",
    "        axis=-1,\n",
    "        name=\"sequence_output\",\n",
    "    )\n",
    "\n",
    "    out = tf.keras.layers.Flatten()(sequence_output)\n",
    "    out = tf.keras.layers.Dense(1, activation=\"sigmoid\")(out)\n",
    "\n",
    "    model = tf.keras.models.Model(\n",
    "        inputs=[\n",
    "            roberta_input_word_ids,\n",
    "            roberta_attention_mask,\n",
    "            roberta_token_type_ids,  # roberta input\n",
    "        ],\n",
    "        outputs=out,\n",
    "    )\n",
    "    model.compile(\n",
    "        tf.keras.optimizers.Adam(lr=5e-5),\n",
    "        loss=\"binary_crossentropy\",\n",
    "        metrics=[tf.keras.metrics.AUC()],\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "model = build_model(max_len=MAX_LEN)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp = 'roberta-baseline' \n",
    "n_splits = 5\n",
    "n_epochs = 5\n",
    "\n",
    "DISPLAY=1 # USE display=1 FOR INTERACTIVE\n",
    "kf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\n",
    "output_dir = f'../{exp}_models'\n",
    "os.makedirs(output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scheduler(epoch):\n",
    "    return 3e-5*0.2**epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for fold, (idxT,idxV) in enumerate(kf.split(train_df)):\n",
    "    print('*'*100)\n",
    "    print(f'FOLD: {fold+1}/{n_splits}')\n",
    "    \n",
    "    K.clear_session()\n",
    "    \n",
    "    model = build_model(max_len=MAX_LEN)\n",
    "        \n",
    "    reduce_lr = tf.keras.callbacks.LearningRateScheduler(scheduler)\n",
    "\n",
    "    model_dir = os.path.join(output_dir, f'Fold_{fold+1}.h5')\n",
    "\n",
    "    sv = tf.keras.callbacks.ModelCheckpoint(model_dir, \n",
    "                                            monitor='val_auc', \n",
    "                                            verbose=1, \n",
    "                                            save_best_only=True,\n",
    "                                            save_weights_only=True, \n",
    "                                            mode='auto', \n",
    "                                            save_freq='epoch')\n",
    "    \n",
    "    train_df_ = train_df.iloc[idxT]\n",
    "    val_df_ = train_df.iloc[idxV]\n",
    "    train_dataset, valid_dataset = data_generator(train_df_, val_df_)\n",
    "    \n",
    "    n_steps = train_df_.shape[0] // BATCH_SIZE\n",
    "    train_history = model.fit(\n",
    "        train_dataset,\n",
    "        steps_per_epoch=n_steps,\n",
    "        \n",
    "        callbacks=[sv, \n",
    "            reduce_lr,\n",
    "            # tb\n",
    "            ],\n",
    "        validation_data=valid_dataset,\n",
    "        epochs=n_epochs\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.predict(valid_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = regular_encode(test_df['post_message'].values, maxlen=MAX_LEN)\n",
    "y_test = np.zeros((len(test_df),1))\n",
    "test_dataset = (\n",
    "    tf.data.Dataset\n",
    "    .from_tensor_slices((X_test,y_test))\n",
    "    .batch(BATCH_SIZE)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(max_len=MAX_LEN)\n",
    "preds = []\n",
    "for i, file_name in enumerate(os.listdir(output_dir)):\n",
    "    print('_'*80)\n",
    "    \n",
    "    K.clear_session()\n",
    "    model_path = os.path.join(output_dir, file_name)\n",
    "    \n",
    "    print(f'Inferencing with model from: {model_path}')\n",
    "    model.load_weights(model_path)\n",
    "\n",
    "    pred = model.predict(test_dataset,\n",
    "                         batch_size=128,\n",
    "                         verbose=DISPLAY)\n",
    "    # print(pred[])\n",
    "    preds.append(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = np.mean(preds, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df[\"prediction\"] = preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df[\"prediction\"].to_csv(\"result.csv\", header=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
