{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Nov 30 14:50:55 2020       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 440.33.01    Driver Version: 440.33.01    CUDA Version: 10.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
      "| N/A   49C    P0    35W /  70W |     11MiB / 15109MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  Tesla T4            Off  | 00000000:00:05.0 Off |                    0 |\n",
      "| N/A   77C    P0    66W /  70W |  12860MiB / 15109MiB |     92%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  Tesla T4            Off  | 00000000:00:06.0 Off |                    0 |\n",
      "| N/A   77C    P0    72W /  70W |  11796MiB / 15109MiB |    100%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  Tesla T4            Off  | 00000000:00:07.0 Off |                    0 |\n",
      "| N/A   50C    P0    36W /  70W |     11MiB / 15109MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                       GPU Memory |\n",
      "|  GPU       PID   Type   Process name                             Usage      |\n",
      "|=============================================================================|\n",
      "|    1     13025      C   python                                     12849MiB |\n",
      "|    2      1642      C   python                                     11785MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "! nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m W&B installed but not logged in.  Run `wandb login` or set the WANDB_API_KEY env variable.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Tensorflow version: 2.3.0\n",
      "Using Transformers version: 3.0.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/leonard/leonard/ai_server_1_anaconda3/envs/tf230/lib/python3.7/site-packages/tqdm/std.py:701: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
      "  from pandas import Panel\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import math\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import os\n",
    "import sys\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from emoji import demojize\n",
    "\n",
    "import random\n",
    "import math\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import string\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split, KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "import transformers\n",
    "from transformers import TFAutoModel, TFRobertaModel, AutoTokenizer, TFAutoModelForSequenceClassification,TFAutoModelForSequenceClassification\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "print('Using Tensorflow version:', tf.__version__)\n",
    "print('Using Transformers version:', transformers.__version__)\n",
    "\n",
    "# import warnings\n",
    "# warnings.filterwarnings('ignore')\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\" \n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1512\n",
    "def seed_all(seed=1512):\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "seed_all(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # %%time\n",
    "\n",
    "# train_df = pd.read_csv(\"/home/leonard/leonard/nlp/ReINTEL/data/final_data/train_5_folds.csv\")\n",
    "# # test_df = pd.read_csv(\"/home/leonard/leonard/nlp/ReINTEL/data/final_data/test.csv\")\n",
    "# test_df = pd.read_csv(\"/home/leonard/leonard/nlp/ReINTEL/data/final_data/private_test.csv\", index_col=0)\n",
    "\n",
    "\n",
    "# train_df[\"post_message\"] = train_df[\"post_message\"].astype(str)\n",
    "# test_df[\"post_message\"] = test_df[\"post_message\"].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# roberta = '/home/leonard/leonard/nlp/ReINTEL/outputs/pretraining_vlsp' \n",
    "# roberta_tokenizer = AutoTokenizer.from_pretrained(roberta)\n",
    "# # roberta_model = TFAutoModel.from_pretrained(roberta)\n",
    "# roberta_model = TFRobertaModel.from_pretrained(roberta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_len_word = [len(text.split()) for text in train_df.post_message]\n",
    "# test_len_word = [len(text.split()) for text in test_df.post_message]\n",
    "# test_len_char = [len(text) for text in train_df.post_message]\n",
    "# test_len_char = [len(text) for text in test_df.post_message]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def length_plot(lengths):\n",
    "#     plt.figure(figsize=(12,7))\n",
    "#     textstr = f' Mean: {np.mean(lengths):.2f} \\u00B1 {np.std(lengths):.2f} \\n Max: {np.max(lengths)}'\n",
    "#     props = dict(boxstyle='round', facecolor='wheat', alpha=0.5)\n",
    "\n",
    "#     plt.text(0, 0, textstr, fontsize=14,\n",
    "#             verticalalignment='top', bbox=props)\n",
    "#     sns.countplot(lengths)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# length_plot(train_len_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# length_plot(test_len_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAX_LEN = 256\n",
    "# BATCH_SIZE = 48"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def regular_encode(texts, maxlen=MAX_LEN):\n",
    "\n",
    "#     roberta_enc_di = roberta_tokenizer.batch_encode_plus(\n",
    "#         texts,\n",
    "#         return_token_type_ids=True,\n",
    "#         pad_to_max_length=True,\n",
    "#         max_length=maxlen,\n",
    "#         truncation=True,\n",
    "#     )\n",
    "    \n",
    "#     roberta_enc = (\n",
    "#         np.array(roberta_enc_di[\"input_ids\"]),\n",
    "#         np.array(roberta_enc_di[\"attention_mask\"]),\n",
    "#         np.array(roberta_enc_di[\"token_type_ids\"]),\n",
    "#     )\n",
    "#     return roberta_enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AUTO = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "# def data_generator(train_df, val_df):\n",
    "\n",
    "#     X_train = regular_encode(train_df[\"post_message\"].values, maxlen=MAX_LEN)\n",
    "#     # y_train = tf.keras.utils.to_categorical(train_df['Label'].values, num_classes=2)\n",
    "#     y_train = train_df[\"label\"].values\n",
    "#     X_val = regular_encode(val_df[\"post_message\"].values, maxlen=MAX_LEN)\n",
    "#     # y_val = tf.keras.utils.to_categorical(val_df['Label'].values, num_classes=2)\n",
    "#     y_val = val_df[\"label\"].values\n",
    "\n",
    "#     train_dataset = (\n",
    "#         tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "#         .repeat()\n",
    "#         .shuffle(1024)\n",
    "#         .batch(BATCH_SIZE)\n",
    "#         .prefetch(AUTO)\n",
    "#     )\n",
    "\n",
    "#     valid_dataset = (\n",
    "#         tf.data.Dataset.from_tensor_slices((X_val, y_val))\n",
    "#         .batch(BATCH_SIZE)\n",
    "#         .cache()\n",
    "#         .prefetch(AUTO)\n",
    "#     )\n",
    "\n",
    "#     return train_dataset, valid_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def build_model(bert_model_name_or_path, max_len=384, n_hiddens=-1):\n",
    "#     bert_model = TFAutoModel.from_pretrained(bert_model_name_or_path)\n",
    "\n",
    "#     bert_input_word_ids = tf.keras.layers.Input(\n",
    "#         shape=(max_len,), dtype=tf.int32, name=\"bert_input_id\"\n",
    "#     )\n",
    "#     bert_attention_mask = tf.keras.layers.Input(\n",
    "#         shape=(max_len,), dtype=tf.int32, name=\"bert_attention_mask\"\n",
    "#     )\n",
    "#     bert_token_type_ids = tf.keras.layers.Input(\n",
    "#         shape=(max_len,), dtype=tf.int32, name=\"bert_token_type_ids\"\n",
    "#     )\n",
    "\n",
    "#     bert_sequence_output = bert_model(\n",
    "#         bert_input_word_ids,\n",
    "#         attention_mask=bert_attention_mask,\n",
    "#         token_type_ids=bert_token_type_ids,\n",
    "# #         output_hidden_states=True,\n",
    "# #         output_attentions=True,\n",
    "        \n",
    "#     )\n",
    "\n",
    "#     # print(len(bert_sequence_output)) # 4\n",
    "\n",
    "#     # print(bert_sequence_output[0].shape) # (None, max_len, 768)\n",
    "\n",
    "#     # print(bert_sequence_output[1].shape) # (None, 768)\n",
    "#     # print(len(bert_sequence_output[2])) # 13\n",
    "#     # print(bert_sequence_output[2][0].shape) # (None, max_len, 768)\n",
    "#     # print(len(bert_sequence_output[3])) # 12\n",
    "#     # print(bert_sequence_output[3][0].shape) # (None, 12, None, max_len)\n",
    "\n",
    "#     # TODO: get bert embedding\n",
    "\n",
    "# #     if n_hiddens == -1:  # get [CLS] token embedding only\n",
    "# #         # print(\"Get pooler output of shape (batch_size, hidden_size)\")\n",
    "# #         bert_sequence_output = bert_sequence_output[0][:, 0, :]\n",
    "# #     else:  # concatenate n_hiddens final layer\n",
    "# #         # print(f\"Concatenate {n_hiddens} hidden_states of shape (batch_size, hidden_size)\")\n",
    "# #         bert_sequence_output = tf.concat(\n",
    "# #             [bert_sequence_output[2][-i] for i in range(n_hiddens)], axis=-1)\n",
    "\n",
    "#     # print(\"bert_sequence_output shape\", bert_sequence_output.shape)\n",
    "\n",
    "    \n",
    "# # MLP    \n",
    "# #     out = tf.keras.layers.Flatten()(bert_sequence_output)\n",
    "# #     out = tf.keras.layers.Dense(1, activation=\"sigmoid\")(out)\n",
    "# # CNN\n",
    "#     out = tf.keras.layers.Dropout(0.15)(bert_sequence_output[0])\n",
    "#     out = tf.keras.layers.Conv1D(768, 2,padding='same')(out)\n",
    "#     out = tf.keras.layers.LeakyReLU()(out)\n",
    "#     out = tf.keras.layers.Conv1D(64, 2,padding='same')(out)\n",
    "# #     out = tf.keras.layers.Dense(1)(out)\n",
    "#     out = tf.keras.layers.Flatten()(out)\n",
    "#     out = tf.keras.layers.Dense(1)(out)\n",
    "#     out = tf.keras.layers.Activation('sigmoid')(out)\n",
    "    \n",
    "    \n",
    "    \n",
    "#     model = tf.keras.models.Model(\n",
    "#         inputs=[\n",
    "#             bert_input_word_ids,\n",
    "#             bert_attention_mask,\n",
    "#             bert_token_type_ids,  # bert input\n",
    "#         ],\n",
    "#         outputs=out,\n",
    "#     )\n",
    "#     model.compile(\n",
    "#         tf.keras.optimizers.Adam(lr=5e-5),\n",
    "#         loss=\"binary_crossentropy\",\n",
    "#         metrics=[tf.keras.metrics.AUC()],\n",
    "#     )\n",
    "\n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# model = build_model(roberta, max_len=MAX_LEN)\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_splits = 5\n",
    "# n_epochs = 5\n",
    "\n",
    "# DISPLAY=1 # USE display=1 FOR INTERACTIVE\n",
    "# exp = f'phobert_cnn_{MAX_LEN}_len'\n",
    "\n",
    "# output_dir = f'../outputs/{exp}_models'\n",
    "# os.makedirs(output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def scheduler(epoch):\n",
    "#     return 3e-5*0.2**epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# strategy = tf.distribute.MirroredStrategy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # for fold, (idxT, idxV) in enumerate(kf.split(train_df)):\n",
    "# for fold in sorted(train_df[\"fold\"].unique()):\n",
    "# #     if fold<3:\n",
    "# #         continue\n",
    "#     print('*'*100)\n",
    "#     print(f'FOLD: {fold+1}/{n_splits}')\n",
    "    \n",
    "#     K.clear_session()\n",
    "#     with strategy.scope():\n",
    "#         model = build_model(roberta, max_len=MAX_LEN)\n",
    "# #     model = build_model(roberta, max_len=MAX_LEN)\n",
    "        \n",
    "#     reduce_lr = tf.keras.callbacks.LearningRateScheduler(scheduler)\n",
    "\n",
    "#     model_dir = os.path.join(output_dir, f'Fold_{fold+1}.h5')\n",
    "\n",
    "#     sv = tf.keras.callbacks.ModelCheckpoint(model_dir, \n",
    "#                                             monitor='val_auc', \n",
    "#                                             verbose=1, \n",
    "#                                             save_best_only=True,\n",
    "#                                             save_weights_only=True, \n",
    "#                                             mode='max', \n",
    "#                                             save_freq='epoch')\n",
    "    \n",
    "#     train_df_ = train_df[train_df[\"fold\"]!=fold]\n",
    "#     val_df_ = train_df[train_df[\"fold\"]==fold]\n",
    "#     train_dataset, valid_dataset = data_generator(train_df_, val_df_)\n",
    "    \n",
    "#     n_steps = train_df_.shape[0] // BATCH_SIZE + 1\n",
    "#     train_history = model.fit(\n",
    "#         train_dataset,\n",
    "#         steps_per_epoch=n_steps,\n",
    "        \n",
    "#         callbacks=[sv, \n",
    "#             reduce_lr,\n",
    "#             # tb\n",
    "#             ],\n",
    "#         validation_data=valid_dataset,\n",
    "#         epochs=n_epochs\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_test = regular_encode(test_df['post_message'].values, maxlen=MAX_LEN)\n",
    "# y_test = np.zeros((len(test_df),1))\n",
    "# test_dataset = (\n",
    "#     tf.data.Dataset\n",
    "#     .from_tensor_slices((X_test,y_test))\n",
    "#     .batch(BATCH_SIZE)\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = build_model(roberta, max_len=MAX_LEN)\n",
    "# preds = []\n",
    "# for i, file_name in enumerate(os.listdir(output_dir)):\n",
    "#     print('_'*80)\n",
    "    \n",
    "#     K.clear_session()\n",
    "#     model_path = os.path.join(output_dir, file_name)\n",
    "    \n",
    "#     print(f'Inferencing with model from: {model_path}')\n",
    "#     model.load_weights(model_path)\n",
    "\n",
    "#     pred = model.predict(test_dataset,\n",
    "#                          batch_size=128,\n",
    "#                          verbose=DISPLAY)\n",
    "#     # print(pred[])\n",
    "#     preds.append(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preds = np.mean(preds, axis=0)\n",
    "# test_df[\"prediction\"] = preds\n",
    "# test_df[\"prediction\"].to_csv(f\"{exp}.csv\", header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "warmup_train_df = pd.read_excel(\"/home/leonard/leonard/my_work/ReINTEL/data/raw_data/warmup_training_dataset.xlsx\", index_col=\"id\")\n",
    "warmup_test_df = pd.read_excel(\"/home/leonard/leonard/my_work/ReINTEL/data/raw_data/warmup_test_set.xlsx\", index_col=\"id\")\n",
    "\n",
    "public_train_df = pd.read_csv(\"/home/leonard/leonard/my_work/ReINTEL/data/raw_data/public_train.csv\")\n",
    "raw_test_df = pd.read_csv(\"/home/leonard/leonard/my_work/ReINTEL/data/raw_data/public_test.csv\")\n",
    "\n",
    "# TODO: make use of warmup_test_df\n",
    "raw_train_df = pd.concat([warmup_train_df, public_train_df]).drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_name</th>\n",
       "      <th>post_message</th>\n",
       "      <th>timestamp_post</th>\n",
       "      <th>num_like_post</th>\n",
       "      <th>num_comment_post</th>\n",
       "      <th>num_share_post</th>\n",
       "      <th>label</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2167074723833126912</td>\n",
       "      <td>Cần các bậc phụ huynh xã Ngũ Thái lên tiếng, k...</td>\n",
       "      <td>1.58443e+09</td>\n",
       "      <td>45</td>\n",
       "      <td>15</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7368497547812410368</td>\n",
       "      <td>KÊU GỌI ĂN CHAY CẦU NGUYỆN XIN CHÚA CỨU KHỎI D...</td>\n",
       "      <td>1.58136e+09</td>\n",
       "      <td>979</td>\n",
       "      <td>39</td>\n",
       "      <td>138</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-5754142625280308224</td>\n",
       "      <td>Giàn khoan dầu khí gặp sự cố, chết người!\\n\\nG...</td>\n",
       "      <td>1.58704e+09</td>\n",
       "      <td>85</td>\n",
       "      <td>13</td>\n",
       "      <td>61</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4177935308849652224</td>\n",
       "      <td>Thuận Lợi có ca dương tính CV19 rồi đó, mọi ng...</td>\n",
       "      <td>1.59645e+09</td>\n",
       "      <td>114</td>\n",
       "      <td>12</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5540309800745996288</td>\n",
       "      <td>Sa Pa cho 9 người khách nước ngoài đi cùng chu...</td>\n",
       "      <td>1.58355e+09</td>\n",
       "      <td>166</td>\n",
       "      <td>4</td>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              user_name                                       post_message  \\\n",
       "0   2167074723833126912  Cần các bậc phụ huynh xã Ngũ Thái lên tiếng, k...   \n",
       "1   7368497547812410368  KÊU GỌI ĂN CHAY CẦU NGUYỆN XIN CHÚA CỨU KHỎI D...   \n",
       "2  -5754142625280308224  Giàn khoan dầu khí gặp sự cố, chết người!\\n\\nG...   \n",
       "3   4177935308849652224  Thuận Lợi có ca dương tính CV19 rồi đó, mọi ng...   \n",
       "4   5540309800745996288  Sa Pa cho 9 người khách nước ngoài đi cùng chu...   \n",
       "\n",
       "  timestamp_post num_like_post num_comment_post num_share_post  label  id  \n",
       "0    1.58443e+09            45               15              8      1 NaN  \n",
       "1    1.58136e+09           979               39            138      1 NaN  \n",
       "2    1.58704e+09            85               13             61      1 NaN  \n",
       "3    1.59645e+09           114               12              5      1 NaN  \n",
       "4    1.58355e+09           166                4             21      1 NaN  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "train_df = pd.read_csv(\"/home/leonard/leonard/my_work/ReINTEL/data/final_data/train_5_folds.csv\")\n",
    "test_df = pd.read_csv(\"/home/leonard/leonard/my_work/ReINTEL/data/final_data/private_test.csv\")\n",
    "# test_df = pd.read_csv(\"/home/leonard/leonard/nlp/ReINTEL/data/final_data/test.csv\")\n",
    "\n",
    "train_df[\"post_message\"] = train_df[\"post_message\"].astype(str)\n",
    "test_df[\"post_message\"] = test_df[\"post_message\"].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenized_text_normalize(text):\n",
    "#     text=  re.sub(r'http(\\S)+', ' ',text)\n",
    "#     text=  re.sub(r'https(\\S)+', ' ',text)\n",
    "#     text=  re.sub(r'http ...', ' ',text)\n",
    "#     text = re.sub(r'@[\\S]+',' ',text)\n",
    "#     text = text.strip(string.punctuation+\" \")\n",
    "#     text = re.sub(\"\\*\", \" \", text)\n",
    "#     text = re.sub(\"#\", \" \", text)\n",
    "#     text = text.strip(r\"# *\" )\n",
    "    text = re.sub(\" _ \",  \" \", text)\n",
    "#     text = re.sub(\"\\.+\",  \"\\.\", text)\n",
    "    text = re.sub(\"…\",  \"...\", text)\n",
    "#     text = re.sub(r\"https?://\\S+|www\\.\\S+\", \"\", text)\n",
    "    text = re.sub(r\"- - -\", \" \", text)\n",
    "    text = re.sub(\"_ \", \" \", text)\n",
    "    text = re.sub(\" +\", \" \", text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenized_text_normalize(input())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[\"post_message\"] = train_df[\"post_message\"].apply(tokenized_text_normalize)\n",
    "test_df[\"post_message\"] = test_df[\"post_message\"].apply(tokenized_text_normalize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"train.txt\", \"w\") as f:\n",
    "    f.write(\"\\n\".join(train_df[\"post_message\"].astype(str)))\n",
    "with open(\"test.txt\", \"w\") as f:\n",
    "    f.write(\"\\n\".join(test_df[\"post_message\"].astype(str)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_name</th>\n",
       "      <th>post_message</th>\n",
       "      <th>timestamp_post</th>\n",
       "      <th>num_like_post</th>\n",
       "      <th>num_comment_post</th>\n",
       "      <th>num_share_post</th>\n",
       "      <th>label</th>\n",
       "      <th>id</th>\n",
       "      <th>fold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.16707472383313E+018</td>\n",
       "      <td>Cần các bậc phụ_huynh xã Ngũ_Thái lên_tiếng , ...</td>\n",
       "      <td>1.584426e+09</td>\n",
       "      <td>45</td>\n",
       "      <td>15</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.36849754781241E+018</td>\n",
       "      <td>KÊU_GỌI ĂN_CHAY CẦU_NGUYỆN XIN CHÚA CỨU KHỎI D...</td>\n",
       "      <td>1.581363e+09</td>\n",
       "      <td>979</td>\n",
       "      <td>39</td>\n",
       "      <td>138</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-5.75414262528031E+018</td>\n",
       "      <td>Giàn khoan dầu_khí gặp sự_cố , chết người ! Gi...</td>\n",
       "      <td>1.587042e+09</td>\n",
       "      <td>85</td>\n",
       "      <td>13</td>\n",
       "      <td>61</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.17793530884965E+018</td>\n",
       "      <td>Thuận_Lợi có ca dương_tính CV19 rồi đó , mọi n...</td>\n",
       "      <td>1.596449e+09</td>\n",
       "      <td>114</td>\n",
       "      <td>12</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.540309800746E+018</td>\n",
       "      <td>Sa_Pa cho 9 người khách nước_ngoài đi cùng chu...</td>\n",
       "      <td>1.583548e+09</td>\n",
       "      <td>166</td>\n",
       "      <td>4</td>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                user_name                                       post_message  \\\n",
       "0   2.16707472383313E+018  Cần các bậc phụ_huynh xã Ngũ_Thái lên_tiếng , ...   \n",
       "1   7.36849754781241E+018  KÊU_GỌI ĂN_CHAY CẦU_NGUYỆN XIN CHÚA CỨU KHỎI D...   \n",
       "2  -5.75414262528031E+018  Giàn khoan dầu_khí gặp sự_cố , chết người ! Gi...   \n",
       "3   4.17793530884965E+018  Thuận_Lợi có ca dương_tính CV19 rồi đó , mọi n...   \n",
       "4     5.540309800746E+018  Sa_Pa cho 9 người khách nước_ngoài đi cùng chu...   \n",
       "\n",
       "   timestamp_post num_like_post num_comment_post num_share_post  label  id  \\\n",
       "0    1.584426e+09            45               15              8      1 NaN   \n",
       "1    1.581363e+09           979               39            138      1 NaN   \n",
       "2    1.587042e+09            85               13             61      1 NaN   \n",
       "3    1.596449e+09           114               12              5      1 NaN   \n",
       "4    1.583548e+09           166                4             21      1 NaN   \n",
       "\n",
       "   fold  \n",
       "0     3  \n",
       "1     1  \n",
       "2     3  \n",
       "3     1  \n",
       "4     3  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_num(text):\n",
    "    if type(text)==str:\n",
    "        if text==\"unknown\":\n",
    "            return math.nan\n",
    "        return re.findall(\"\\d+\", text)[0]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in [\"timestamp_post\", \"num_like_post\", \"num_comment_post\", \"num_share_post\"]:\n",
    "    train_df[col] = train_df[col].apply(extract_num)\n",
    "    train_df[col] = train_df[col].astype(float)\n",
    "    train_df[col] = train_df[col].fillna(-train_df[col].mean())\n",
    "    test_df[col] = test_df[col].apply(extract_num)\n",
    "    test_df[col] = test_df[col].astype(float)\n",
    "    test_df[col] = test_df[col].fillna(-test_df[col].mean())    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nan_dict = {\"timestamp_post\": 1e10, \"num_like_post\":-2,\"num_comment_post\":-2, \"num_share_post\":-2}\n",
    "# unknown_dict = {\"timestamp_post\": 1e10, \"num_like_post\":-2,\"num_comment_post\":-2, \"num_share_post\":-2}\n",
    "\n",
    "\n",
    "# def vlsp_impute(text, field):\n",
    "#     if type(text)!=str:\n",
    "#         if math.isnan(text):\n",
    "#             return nan_dict[field]\n",
    "#     elif text==\"unknown\":        \n",
    "#         return unknown_dict[field]\n",
    "#     else:\n",
    "#         try:\n",
    "#             return int(extract_num(text))\n",
    "#         except:\n",
    "#             print(text)\n",
    "#             return nan_dict[field]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaler = StandardScaler()\n",
    "# scaler.fit(pd.concat([train_df, test_df])[[\"timestamp_post\", \"num_like_post\", \"num_comment_post\", \"num_share_post\"]].values)\n",
    "# train_df[[\"timestamp_post\", \"num_like_post\", \"num_comment_post\", \"num_share_post\"]] = scaler.transform(train_df[[\"timestamp_post\", \"num_like_post\", \"num_comment_post\", \"num_share_post\"]])\n",
    "# test_df[[\"timestamp_post\", \"num_like_post\", \"num_comment_post\", \"num_share_post\"]] = scaler.transform(test_df[[\"timestamp_post\", \"num_like_post\", \"num_comment_post\", \"num_share_post\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df[\"timestamp_post\"] = train_df[\"timestamp_post\"].fillna(-1e10)\n",
    "# test_df[\"timestamp_post\"] = test_df[\"timestamp_post\"].fillna(-1e10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Model name '/home/leonard/leonard/my_work/ReINTEL/pretrained_phobert-base' was not found in tokenizers model name list (roberta-base, roberta-large, roberta-large-mnli, distilroberta-base, roberta-base-openai-detector, roberta-large-openai-detector). We assumed '/home/leonard/leonard/my_work/ReINTEL/pretrained_phobert-base' was a path, a model identifier, or url to a directory containing vocabulary files named ['vocab.json', 'merges.txt'] but couldn't find such vocabulary files at this path or url.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-43-78df2e2f79b0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m48\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mroberta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'vinai/phobert-base'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mroberta_tokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/home/leonard/leonard/my_work/ReINTEL/pretrained_phobert-base\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_fast\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/leonard/ai_server_1_anaconda3/envs/tf230/lib/python3.7/site-packages/transformers/tokenization_auto.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    215\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mtokenizer_class_fast\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mtokenizer_class_py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m         raise ValueError(\n",
      "\u001b[0;32m~/leonard/ai_server_1_anaconda3/envs/tf230/lib/python3.7/site-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m   1138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1139\u001b[0m         \"\"\"\n\u001b[0;32m-> 1140\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_from_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1142\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/leonard/ai_server_1_anaconda3/envs/tf230/lib/python3.7/site-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36m_from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   1244\u001b[0m                     \u001b[0;34m\", \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms3_models\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1245\u001b[0m                     \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1246\u001b[0;31m                     \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab_files_names\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1247\u001b[0m                 )\n\u001b[1;32m   1248\u001b[0m             )\n",
      "\u001b[0;31mOSError\u001b[0m: Model name '/home/leonard/leonard/my_work/ReINTEL/pretrained_phobert-base' was not found in tokenizers model name list (roberta-base, roberta-large, roberta-large-mnli, distilroberta-base, roberta-base-openai-detector, roberta-large-openai-detector). We assumed '/home/leonard/leonard/my_work/ReINTEL/pretrained_phobert-base' was a path, a model identifier, or url to a directory containing vocabulary files named ['vocab.json', 'merges.txt'] but couldn't find such vocabulary files at this path or url."
     ]
    }
   ],
   "source": [
    "MAX_LEN = 256\n",
    "BATCH_SIZE = 48\n",
    "roberta = 'vinai/phobert-base' \n",
    "roberta_tokenizer = AutoTokenizer.from_pretrained(\"/home/leonard/leonard/my_work/ReINTEL/pretrained_phobert-base\", use_fast=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regular_encode(df, max_len=MAX_LEN):\n",
    "    \n",
    "    roberta_enc_di = roberta_tokenizer.batch_encode_plus(\n",
    "        df[\"post_message\"].values,\n",
    "        return_token_type_ids=True,\n",
    "        pad_to_max_length=True,\n",
    "        max_length=max_len,\n",
    "        truncation=True,\n",
    "    )\n",
    "    timestamp_post = (df[\"timestamp_post\"]/1e13).values\n",
    "    num_like_post = (df[\"num_like_post\"]/1e8).values\n",
    "    num_comment_post = (df[\"num_comment_post\"]/1e8).values\n",
    "    num_share_post = (df[\"num_share_post\"]/1e8).values\n",
    "    \n",
    "    roberta_enc = (\n",
    "        np.array(roberta_enc_di[\"input_ids\"]),\n",
    "        np.array(roberta_enc_di[\"attention_mask\"]),\n",
    "        np.array(roberta_enc_di[\"token_type_ids\"]),\n",
    "        timestamp_post,\n",
    "        num_like_post,\n",
    "        num_comment_post,\n",
    "        num_share_post,\n",
    "#         np.concatenate([num_like_post, num_comment_post, num_share_post], axis=0)\n",
    "    )\n",
    "    \n",
    "    \n",
    "    return roberta_enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTO = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "def data_generator(train_df, val_df):\n",
    "\n",
    "    X_train = regular_encode(train_df, max_len=MAX_LEN)\n",
    "    # y_train = tf.keras.utils.to_categorical(train_df['Label'].values, num_classes=2)\n",
    "    y_train = train_df[\"label\"].values\n",
    "    X_val = regular_encode(val_df, max_len=MAX_LEN)\n",
    "    # y_val = tf.keras.utils.to_categorical(val_df['Label'].values, num_classes=2)\n",
    "    y_val = val_df[\"label\"].values\n",
    "\n",
    "    train_dataset = (\n",
    "        tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "        .repeat()\n",
    "        .shuffle(1024)\n",
    "        .batch(BATCH_SIZE)\n",
    "        .prefetch(AUTO)\n",
    "    )\n",
    "\n",
    "    valid_dataset = (\n",
    "        tf.data.Dataset.from_tensor_slices((X_val, y_val))\n",
    "        .batch(BATCH_SIZE)\n",
    "        .cache()\n",
    "        .prefetch(AUTO)\n",
    "    )\n",
    "\n",
    "    return train_dataset, valid_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(bert_model_name_or_path=\"vinai/phobert-base\", max_len=384, n_hiddens=-1):\n",
    "    bert_model = TFAutoModel.from_pretrained(bert_model_name_or_path)\n",
    "\n",
    "    bert_input_word_ids = tf.keras.layers.Input(\n",
    "        shape=(max_len,), dtype=tf.int32, name=\"bert_input_id\"\n",
    "    )\n",
    "    bert_attention_mask = tf.keras.layers.Input(\n",
    "        shape=(max_len,), dtype=tf.int32, name=\"bert_attention_mask\"\n",
    "    )\n",
    "    bert_token_type_ids = tf.keras.layers.Input(\n",
    "        shape=(max_len,), dtype=tf.int32, name=\"bert_token_type_ids\"\n",
    "    )\n",
    "\n",
    "    bert_sequence_output = bert_model(\n",
    "        bert_input_word_ids,\n",
    "        attention_mask=bert_attention_mask,\n",
    "        token_type_ids=bert_token_type_ids,\n",
    "        output_hidden_states=True,\n",
    "        output_attentions=True,\n",
    "        \n",
    "    )\n",
    "\n",
    "    # print(len(bert_sequence_output)) # 4\n",
    "\n",
    "    # print(bert_sequence_output[0].shape) # (None, max_len, 768)\n",
    "\n",
    "    # print(bert_sequence_output[1].shape) # (None, 768)\n",
    "    # print(len(bert_sequence_output[2])) # 13\n",
    "    # print(bert_sequence_output[2][0].shape) # (None, max_len, 768)\n",
    "    # print(len(bert_sequence_output[3])) # 12\n",
    "    # print(bert_sequence_output[3][0].shape) # (None, 12, None, max_len)\n",
    "\n",
    "    # TODO: get bert embedding\n",
    "\n",
    "    if n_hiddens == -1:  # get [CLS] token embedding only\n",
    "        # print(\"Get pooler output of shape (batch_size, hidden_size)\")\n",
    "        bert_sequence_output = bert_sequence_output[0][:, 0, :]\n",
    "#         bert_sequence_output = bert_sequence_output[1]\n",
    "    else:  # concatenate n_hiddens final layer\n",
    "        # print(f\"Concatenate {n_hiddens} hidden_states of shape (batch_size, hidden_size)\")\n",
    "        bert_sequence_output = tf.concat(\n",
    "            [bert_sequence_output[2][-i] for i in range(n_hiddens)], axis=-1)\n",
    "        bert_sequence_output = bert_sequence_output[:, 0, :]\n",
    "\n",
    "    # print(\"bert_sequence_output shape\", bert_sequence_output.shape)\n",
    "\n",
    "#     bert_output = tf.keras.layers.Flatten()(bert_sequence_output)\n",
    "    bert_output = tf.keras.layers.Dense(8, activation=\"relu\")(bert_sequence_output)\n",
    "#     print(bert_output.shape)\n",
    "    \n",
    "    timestamp_post = tf.keras.layers.Input(shape=(1,), dtype=tf.float32, name=\"timestamp_post\")\n",
    "    num_like_post = tf.keras.layers.Input(shape=(1,), dtype=tf.float32, name=\"num_like_post\")\n",
    "    num_comment_post = tf.keras.layers.Input(shape=(1,), dtype=tf.float32, name=\"num_comment_post\")\n",
    "    num_share_post = tf.keras.layers.Input(shape=(1,), dtype=tf.float32, name=\"num_share_post\")\n",
    "    \n",
    "    aulixiary_info = tf.keras.layers.Concatenate()([timestamp_post, num_like_post, num_comment_post, num_share_post])\n",
    "#     aulixiary_output  = tf.keras.layers.GaussianNoise(0.2)(aulixiary_info)\n",
    "    \n",
    "    out = tf.keras.layers.Concatenate()([bert_output, aulixiary_info]) \n",
    "#     print(out.shape)\n",
    "    out = tf.keras.layers.Dense(1, activation=\"sigmoid\")(out)\n",
    "\n",
    "    model = tf.keras.models.Model(\n",
    "        inputs=[\n",
    "            bert_input_word_ids,\n",
    "            bert_attention_mask,\n",
    "            bert_token_type_ids,  # bert input\n",
    "            timestamp_post,\n",
    "            num_like_post,\n",
    "            num_comment_post,\n",
    "            num_share_post,\n",
    "        ],\n",
    "        outputs=out,\n",
    "    )\n",
    "    model.compile(\n",
    "        tf.keras.optimizers.Adam(lr=5e-5),\n",
    "        loss=\"binary_crossentropy\",\n",
    "        metrics=[tf.keras.metrics.AUC()],\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at vinai/phobert-base were not used when initializing TFRobertaModel: ['lm_head']\n",
      "- This IS expected if you are initializing TFRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFRobertaModel were initialized from the model checkpoint at vinai/phobert-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "bert_input_id (InputLayer)      [(None, 256)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bert_attention_mask (InputLayer [(None, 256)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bert_token_type_ids (InputLayer [(None, 256)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_roberta_model (TFRobertaMode ((None, 256, 768), ( 134998272   bert_input_id[0][0]              \n",
      "                                                                 bert_attention_mask[0][0]        \n",
      "                                                                 bert_token_type_ids[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice (Tens [(None, 768)]        0           tf_roberta_model[0][14]          \n",
      "__________________________________________________________________________________________________\n",
      "timestamp_post (InputLayer)     [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "num_like_post (InputLayer)      [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "num_comment_post (InputLayer)   [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "num_share_post (InputLayer)     [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 8)            6152        tf_op_layer_strided_slice[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 4)            0           timestamp_post[0][0]             \n",
      "                                                                 num_like_post[0][0]              \n",
      "                                                                 num_comment_post[0][0]           \n",
      "                                                                 num_share_post[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 12)           0           dense[0][0]                      \n",
      "                                                                 concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 1)            13          concatenate_1[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 135,004,437\n",
      "Trainable params: 135,004,437\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "CPU times: user 5.26 s, sys: 1.03 s, total: 6.3 s\n",
      "Wall time: 8.65 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = build_model(max_len=MAX_LEN)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_splits = 5\n",
    "n_epochs = 5\n",
    "\n",
    "DISPLAY=1 # USE display=1 FOR INTERACTIVE\n",
    "exp = f'phobert+auxiliary_{MAX_LEN}_len'\n",
    "\n",
    "output_dir = f'../outputs/{exp}_models'\n",
    "os.makedirs(output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scheduler(epoch):\n",
    "    return 3e-5*0.2**epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1')\n"
     ]
    }
   ],
   "source": [
    "strategy = tf.distribute.MirroredStrategy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************************************************************************************\n",
      "FOLD: 1/5\n",
      "****************************************************************************************************\n",
      "FOLD: 2/5\n",
      "****************************************************************************************************\n",
      "FOLD: 3/5\n",
      "****************************************************************************************************\n",
      "FOLD: 4/5\n",
      "****************************************************************************************************\n",
      "FOLD: 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at vinai/phobert-base were not used when initializing TFRobertaModel: ['lm_head']\n",
      "- This IS expected if you are initializing TFRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFRobertaModel were initialized from the model checkpoint at vinai/phobert-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n",
      "/home/leonard/leonard/ai_server_1_anaconda3/envs/tf230/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "WARNING:tensorflow:From /home/leonard/leonard/ai_server_1_anaconda3/envs/tf230/lib/python3.7/site-packages/tensorflow/python/data/ops/multi_device_iterator_ops.py:601: get_next_as_optional (from tensorflow.python.data.ops.iterator_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Iterator.get_next_as_optional()` instead.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
      "INFO:tensorflow:batch_all_reduce: 198 all-reduces with algorithm = nccl, num_packs = 1\n",
      "WARNING:tensorflow:Efficient allreduce is not supported for 3 IndexedSlices\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1').\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1').\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1').\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
      "INFO:tensorflow:batch_all_reduce: 198 all-reduces with algorithm = nccl, num_packs = 1\n",
      "WARNING:tensorflow:Efficient allreduce is not supported for 3 IndexedSlices\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1').\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1').\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1').\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "83/83 [==============================] - ETA: 0s - loss: 0.4744 - auc: 0.6892\n",
      "Epoch 00001: val_auc improved from -inf to 0.91196, saving model to ../outputs/phobert+auxiliary_256_len_models/Fold_5.h5\n",
      "83/83 [==============================] - 127s 2s/step - loss: 0.4744 - auc: 0.6892 - val_loss: 0.2690 - val_auc: 0.9120\n",
      "Epoch 2/5\n",
      "83/83 [==============================] - ETA: 0s - loss: 0.2484 - auc: 0.9367\n",
      "Epoch 00002: val_auc improved from 0.91196 to 0.93829, saving model to ../outputs/phobert+auxiliary_256_len_models/Fold_5.h5\n",
      "83/83 [==============================] - 125s 2s/step - loss: 0.2484 - auc: 0.9367 - val_loss: 0.2398 - val_auc: 0.9383\n",
      "Epoch 3/5\n",
      "83/83 [==============================] - ETA: 0s - loss: 0.2054 - auc: 0.9539\n",
      "Epoch 00003: val_auc improved from 0.93829 to 0.94483, saving model to ../outputs/phobert+auxiliary_256_len_models/Fold_5.h5\n",
      "83/83 [==============================] - 126s 2s/step - loss: 0.2054 - auc: 0.9539 - val_loss: 0.2301 - val_auc: 0.9448\n",
      "Epoch 4/5\n",
      "83/83 [==============================] - ETA: 0s - loss: 0.1900 - auc: 0.9632\n",
      "Epoch 00004: val_auc improved from 0.94483 to 0.94541, saving model to ../outputs/phobert+auxiliary_256_len_models/Fold_5.h5\n",
      "83/83 [==============================] - 127s 2s/step - loss: 0.1900 - auc: 0.9632 - val_loss: 0.2251 - val_auc: 0.9454\n",
      "Epoch 5/5\n",
      "74/83 [=========================>....] - ETA: 12s - loss: 0.1863 - auc: 0.9638"
     ]
    }
   ],
   "source": [
    "# for fold, (idxT, idxV) in enumerate(kf.split(train_df)):\n",
    "for fold in sorted(train_df[\"fold\"].unique()):\n",
    "    print('*'*100)\n",
    "    print(f'FOLD: {fold+1}/{n_splits}')\n",
    "    if fold<4:\n",
    "        continue\n",
    "    K.clear_session()\n",
    "    with strategy.scope():\n",
    "        model = build_model(max_len=MAX_LEN)\n",
    "        \n",
    "    reduce_lr = tf.keras.callbacks.LearningRateScheduler(scheduler)\n",
    "\n",
    "    model_dir = os.path.join(output_dir, f'Fold_{fold+1}.h5')\n",
    "\n",
    "    sv = tf.keras.callbacks.ModelCheckpoint(model_dir, \n",
    "                                            monitor='val_auc', \n",
    "                                            verbose=1, \n",
    "                                            save_best_only=True,\n",
    "                                            save_weights_only=True, \n",
    "                                            mode='max', \n",
    "                                            save_freq='epoch')\n",
    "    \n",
    "    train_df_ = train_df[train_df[\"fold\"]!=fold]\n",
    "    val_df_ = train_df[train_df[\"fold\"]==fold]\n",
    "    train_dataset, valid_dataset = data_generator(train_df_, val_df_)\n",
    "    \n",
    "    n_steps = train_df_.shape[0] // BATCH_SIZE + 1\n",
    "    train_history = model.fit(\n",
    "        train_dataset,\n",
    "        steps_per_epoch=n_steps,\n",
    "        \n",
    "        callbacks=[sv, \n",
    "            reduce_lr,\n",
    "            # tb\n",
    "            ],\n",
    "        validation_data=valid_dataset,\n",
    "        epochs=n_epochs\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0.94458 0.94645 0.95718 0.94500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = regular_encode(test_df, max_len=MAX_LEN)\n",
    "y_test = np.zeros((len(test_df),1))\n",
    "test_dataset = (\n",
    "    tf.data.Dataset\n",
    "    .from_tensor_slices((X_test,y_test))\n",
    "    .batch(BATCH_SIZE)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(max_len=MAX_LEN)\n",
    "preds = []\n",
    "for i, file_name in enumerate(os.listdir(output_dir)):\n",
    "    print('_'*80)\n",
    "    \n",
    "    K.clear_session()\n",
    "    model_path = os.path.join(output_dir, file_name)\n",
    "    \n",
    "    print(f'Inferencing with model from: {model_path}')\n",
    "    model.load_weights(model_path)\n",
    "\n",
    "    pred = model.predict(test_dataset,\n",
    "                         batch_size=128,\n",
    "                         verbose=DISPLAY)\n",
    "    # print(pred[])\n",
    "    preds.append(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = np.mean(preds, axis=0)\n",
    "test_df[\"prediction\"] = preds\n",
    "test_df[\"prediction\"].to_csv(f\"{exp}.csv\", header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pretraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from data.tokenizer import VnCoreTokenizer\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "vncoretokenizer = VnCoreTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_normalize(text):\n",
    "    text = text.strip()\n",
    "    text = re.sub(\"^TTO *- *\", \"\", text)    \n",
    "    text = re.sub(\"^VOV\\.VN *- *\", \"\", text)\n",
    "#     text = vncoretokenizer.tokenize(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_df = pd.read_csv(\"/home/leonard/leonard/nlp/ReINTEL/data/newdata_21112020.csv\")[[\"title\", \"content\"]].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_df[\"content\"] = news_df[\"content\"].apply(text_normalize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_df[\"title\"] = news_df[\"title\"].apply(text_normalize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# news_df.to_csv(\"/home/leonard/leonard/nlp/ReINTEL/data/tokenized_news_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# news_df = pd.read_csv(\"/home/leonard/leonard/nlp/ReINTEL/data/tokenized_news_data.csv\")\n",
    "# news_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_documents = []\n",
    "\n",
    "for index, row in tqdm(news_df.iterrows(), total=len(news_df)):\n",
    "    title = vncoretokenizer.tokenize(str(row[\"title\"]))\n",
    "    text = str(row[\"content\"])\n",
    "    sentences = vncoretokenizer.tokenize(text, return_sentences=True)\n",
    "    document = [title] + sentences\n",
    "    all_documents.append(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.shuffle(all_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_documents = all_documents[:int(len(all_documents)*0.8)]\n",
    "val_documents = all_documents[int(len(all_documents)*0.8):]\n",
    "print(len(train_documents), len(val_documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"/home/leonard/leonard/nlp/ReINTEL/data/tokenized_news_train.txt\", \"w\") as f:\n",
    "#     for document in tqdm(train_documents):\n",
    "#         for sentence in document:\n",
    "#             f.write(sentence)\n",
    "#             f.write(\"\\n\")\n",
    "#         f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"/home/leonard/leonard/nlp/ReINTEL/data/tokenized_news_val.txt\", \"w\") as f:\n",
    "#     for document in tqdm(val_documents):\n",
    "#         for sentence in document:\n",
    "#             f.write(sentence)\n",
    "#             f.write(\"\\n\")\n",
    "#         f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warmup_train_df = pd.read_excel(\"/home/leonard/leonard/nlp/ReINTEL/data/raw_data/warmup_training_dataset.xlsx\", index_col=\"id\")\n",
    "warmup_test_df = pd.read_excel(\"/home/leonard/leonard/nlp/ReINTEL/data/raw_data/warmup_test_set.xlsx\", index_col=\"id\")\n",
    "\n",
    "public_train_df = pd.read_csv(\"/home/leonard/leonard/nlp/ReINTEL/data/raw_data/public_train.csv\")\n",
    "test_df = pd.read_csv(\"/home/leonard/leonard/nlp/ReINTEL/data/raw_data/public_test.csv\")\n",
    "\n",
    "# TODO: make use of warmup_test_df\n",
    "train_df = pd.concat([warmup_train_df, public_train_df]).drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_documents = train_df[\"post_message\"].values\n",
    "test_documents = test_df[\"post_message\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_all_documents = []\n",
    "\n",
    "for index, row in tqdm(train_df.iterrows(), total=len(train_df)):\n",
    "    text = str(row[\"post_message\"])\n",
    "    sentences = vncoretokenizer.tokenize(text, return_sentences=True)\n",
    "    train_all_documents.append(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.shuffle(train_all_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_all_documents = []\n",
    "\n",
    "for index, row in tqdm(test_df.iterrows(), total=len(test_df)):\n",
    "    text = str(row[\"post_message\"])\n",
    "    sentences = vncoretokenizer.tokenize(text, return_sentences=True)\n",
    "    test_all_documents.append(sentences)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = len(train_all_documents) + len(test_all_documents )\n",
    "test_size = int(total*0.2)\n",
    "train_vlsp_text = test_all_documents + train_all_documents[:-test_size]\n",
    "val_vlsp_text = train_all_documents[-test_size:]\n",
    "print(len(train_vlsp_text), len(val_vlsp_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/home/leonard/leonard/nlp/ReINTEL/data/tokenized_train_vlsp_text.txt\", \"w\") as f:\n",
    "    for document in tqdm(train_vlsp_text):\n",
    "        for sentence in document:\n",
    "            f.write(sentence)\n",
    "            f.write(\"\\n\")\n",
    "        f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/home/leonard/leonard/nlp/ReINTEL/data/tokenized_val_vlsp_text.txt\", \"w\") as f:\n",
    "    for document in tqdm(val_vlsp_text):\n",
    "        for sentence in document:\n",
    "            f.write(sentence)\n",
    "            f.write(\"\\n\")\n",
    "        f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RobertaForMaskedLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import TFAutoModel, AutoTokenizer, TFAutoModelForMaskedLM\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 128\n",
    "BATCH_SIZE = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrain_model = 'vinai/phobert-base'\n",
    "tokenizer =  AutoTokenizer.from_pretrained(pretrain_model)\n",
    "model = TFAutoModelForMaskedLM.from_pretrained(pretrain_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    print(f'Using pretrained {pretrain_model}')\n",
    "    model = TFAutoModelForMaskedLM.from_pretrained(pretrain_model)\n",
    "    optimizer = tf.keras.optimizers.Adam(lr=5e-5)\n",
    "    loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "    metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
    "\n",
    "    model.compile(optimizer, loss=loss, metrics=[\"accuracy\"])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = build_model()\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import LineByLineTextDataset\n",
    "\n",
    "dataset = LineByLineTextDataset(\n",
    "    tokenizer=tokenizer,\n",
    "    file_path=\"/home/leonard/leonard/nlp/ReINTEL/data/tokenized_news_train.txt\",\n",
    "    block_size=128,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=True, mlm_probability=0.15\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"/home/leonard/leonard/nlp/ReINTEL/outputs\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=1,\n",
    "    per_gpu_train_batch_size=64,\n",
    "    save_steps=10_000,\n",
    "    save_total_limit=2,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=dataset,\n",
    "    prediction_loss_only=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## post process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_prediction = pd.read_csv(\"/home/leonard/leonard/nlp/ReINTEL/vlsp_reintel_text/results.csv\", header=None)\n",
    "test_df = pd.read_csv(\"/home/leonard/leonard/nlp/ReINTEL/data/raw_data/public_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_prediction.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df[\"prediction\"] = best_prediction[1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cutoff(prob):\n",
    "    if prob>0.92:\n",
    "        return 1.0\n",
    "    return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for index, row in test_df.iterrows():\n",
    "#     if \"url\" in str(row[\"post_message\"]).lower() and len(str(row[\"post_message\"])) < 20:\n",
    "#         print(\"*\"*50)\n",
    "#         print(row[\"post_message\"])\n",
    "#         print(test_df.loc[index, \"prediction\"])\n",
    "# #         test_df.loc[index, \"prediction\"] = 0.005\n",
    "# #         print(test_df.loc[index, \"prediction\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df[\"prediction\"].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df[\"prediction\"].apply(cutoff).to_csv(f\"post_process.csv\", header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df[\"prediction\"].apply(cutoff).plot()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
