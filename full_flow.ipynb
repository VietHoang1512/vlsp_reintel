{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import math\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import os\n",
    "\n",
    "import random\n",
    "import math\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import string\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split, KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "import transformers\n",
    "from transformers import TFAutoModel, TFRobertaModel, AutoTokenizer, TFAutoModelForSequenceClassification,TFAutoModelForSequenceClassification\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "print('Using Tensorflow version:', tf.__version__)\n",
    "print('Using Transformers version:', transformers.__version__)\n",
    "\n",
    "# import warnings\n",
    "# warnings.filterwarnings('ignore')\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1512\n",
    "def seed_all(seed=1512):\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "seed_all(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "train_df = pd.read_csv(\"/home/leonard/leonard/nlp/ReINTEL/data/final_data/train_5_folds.csv\")\n",
    "test_df = pd.read_csv(\"/home/leonard/leonard/nlp/ReINTEL/data/final_data/private_test.csv\")\n",
    "# test_df = pd.read_csv(\"/home/leonard/leonard/nlp/ReINTEL/data/final_data/test.csv\")\n",
    "\n",
    "train_df[\"post_message\"] = train_df[\"post_message\"].astype(str)\n",
    "test_df[\"post_message\"] = test_df[\"post_message\"].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenized_text_normalize(text):\n",
    "#     text=  re.sub(r'http(\\S)+', ' ',text)\n",
    "#     text=  re.sub(r'https(\\S)+', ' ',text)\n",
    "#     text=  re.sub(r'http ...', ' ',text)\n",
    "#     text = re.sub(r'@[\\S]+',' ',text)\n",
    "#     text = text.strip(string.punctuation+\" \")\n",
    "#     text = re.sub(\"\\*\", \" \", text)\n",
    "#     text = re.sub(\"#\", \" \", text)\n",
    "#     text = text.strip(r\"# *\" )\n",
    "    text = re.sub(\" _ \",  \" \", text)\n",
    "#     text = re.sub(\"\\.+\",  \"\\.\", text)\n",
    "    text = re.sub(\"â€¦\",  \"...\", text)\n",
    "#     text = re.sub(r\"https?://\\S+|www\\.\\S+\", \"\", text)\n",
    "    text = re.sub(r\"- - -\", \" \", text)\n",
    "    text = re.sub(\"_ \", \" \", text)\n",
    "    text = re.sub(\" +\", \" \", text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[\"post_message\"] = train_df[\"post_message\"].apply(tokenized_text_normalize)\n",
    "test_df[\"post_message\"] = test_df[\"post_message\"].apply(tokenized_text_normalize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_num(text):\n",
    "    if type(text)==str:\n",
    "        if text==\"unknown\":\n",
    "            return math.nan\n",
    "        return re.findall(\"\\d+\", text)[0]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in [\"timestamp_post\", \"num_like_post\", \"num_comment_post\", \"num_share_post\"]:\n",
    "    train_df[col] = train_df[col].apply(extract_num)\n",
    "    train_df[col] = train_df[col].astype(float)\n",
    "    train_df[col] = train_df[col].fillna(-train_df[col].mean())\n",
    "    test_df[col] = test_df[col].apply(extract_num)\n",
    "    test_df[col] = test_df[col].astype(float)\n",
    "    test_df[col] = test_df[col].fillna(-test_df[col].mean())    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 256\n",
    "BATCH_SIZE = 48\n",
    "roberta = 'vinai/phobert-base' \n",
    "roberta_tokenizer = AutoTokenizer.from_pretrained(roberta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regular_encode(df, max_len=MAX_LEN):\n",
    "    \n",
    "    roberta_enc_di = roberta_tokenizer.batch_encode_plus(\n",
    "        df[\"post_message\"].values,\n",
    "        return_token_type_ids=True,\n",
    "        pad_to_max_length=True,\n",
    "        max_length=max_len,\n",
    "        truncation=True,\n",
    "    )\n",
    "    timestamp_post = (df[\"timestamp_post\"]/1e13).values\n",
    "    num_like_post = (df[\"num_like_post\"]/1e8).values\n",
    "    num_comment_post = (df[\"num_comment_post\"]/1e8).values\n",
    "    num_share_post = (df[\"num_share_post\"]/1e8).values\n",
    "    \n",
    "    roberta_enc = (\n",
    "        np.array(roberta_enc_di[\"input_ids\"]),\n",
    "        np.array(roberta_enc_di[\"attention_mask\"]),\n",
    "        np.array(roberta_enc_di[\"token_type_ids\"]),\n",
    "        timestamp_post,\n",
    "        num_like_post,\n",
    "        num_comment_post,\n",
    "        num_share_post,\n",
    "#         np.concatenate([num_like_post, num_comment_post, num_share_post], axis=0)\n",
    "    )\n",
    "    \n",
    "    \n",
    "    return roberta_enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTO = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "def data_generator(train_df, val_df):\n",
    "\n",
    "    X_train = regular_encode(train_df, max_len=MAX_LEN)\n",
    "    # y_train = tf.keras.utils.to_categorical(train_df['Label'].values, num_classes=2)\n",
    "    y_train = train_df[\"label\"].values\n",
    "    X_val = regular_encode(val_df, max_len=MAX_LEN)\n",
    "    # y_val = tf.keras.utils.to_categorical(val_df['Label'].values, num_classes=2)\n",
    "    y_val = val_df[\"label\"].values\n",
    "\n",
    "    train_dataset = (\n",
    "        tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "        .repeat()\n",
    "        .shuffle(1024)\n",
    "        .batch(BATCH_SIZE)\n",
    "        .prefetch(AUTO)\n",
    "    )\n",
    "\n",
    "    valid_dataset = (\n",
    "        tf.data.Dataset.from_tensor_slices((X_val, y_val))\n",
    "        .batch(BATCH_SIZE)\n",
    "        .cache()\n",
    "        .prefetch(AUTO)\n",
    "    )\n",
    "\n",
    "    return train_dataset, valid_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(bert_model_name_or_path=\"vinai/phobert-base\", max_len=384, n_hiddens=-1):\n",
    "    bert_model = TFAutoModel.from_pretrained(bert_model_name_or_path)\n",
    "\n",
    "    bert_input_word_ids = tf.keras.layers.Input(\n",
    "        shape=(max_len,), dtype=tf.int32, name=\"bert_input_id\"\n",
    "    )\n",
    "    bert_attention_mask = tf.keras.layers.Input(\n",
    "        shape=(max_len,), dtype=tf.int32, name=\"bert_attention_mask\"\n",
    "    )\n",
    "    bert_token_type_ids = tf.keras.layers.Input(\n",
    "        shape=(max_len,), dtype=tf.int32, name=\"bert_token_type_ids\"\n",
    "    )\n",
    "\n",
    "    bert_sequence_output = bert_model(\n",
    "        bert_input_word_ids,\n",
    "        attention_mask=bert_attention_mask,\n",
    "        token_type_ids=bert_token_type_ids,\n",
    "        output_hidden_states=True,\n",
    "        output_attentions=True,\n",
    "        \n",
    "    )\n",
    "\n",
    "    # print(len(bert_sequence_output)) # 4\n",
    "\n",
    "    # print(bert_sequence_output[0].shape) # (None, max_len, 768)\n",
    "\n",
    "    # print(bert_sequence_output[1].shape) # (None, 768)\n",
    "    # print(len(bert_sequence_output[2])) # 13\n",
    "    # print(bert_sequence_output[2][0].shape) # (None, max_len, 768)\n",
    "    # print(len(bert_sequence_output[3])) # 12\n",
    "    # print(bert_sequence_output[3][0].shape) # (None, 12, None, max_len)\n",
    "\n",
    "    # TODO: get bert embedding\n",
    "\n",
    "    if n_hiddens == -1:  # get [CLS] token embedding only\n",
    "        # print(\"Get pooler output of shape (batch_size, hidden_size)\")\n",
    "        bert_sequence_output = bert_sequence_output[0][:, 0, :]\n",
    "#         bert_sequence_output = bert_sequence_output[1]\n",
    "    else:  # concatenate n_hiddens final layer\n",
    "        # print(f\"Concatenate {n_hiddens} hidden_states of shape (batch_size, hidden_size)\")\n",
    "        bert_sequence_output = tf.concat(\n",
    "            [bert_sequence_output[2][-i] for i in range(n_hiddens)], axis=-1)\n",
    "        bert_sequence_output = bert_sequence_output[:, 0, :]\n",
    "\n",
    "    # print(\"bert_sequence_output shape\", bert_sequence_output.shape)\n",
    "\n",
    "#     bert_output = tf.keras.layers.Flatten()(bert_sequence_output)\n",
    "    bert_output = tf.keras.layers.Dense(8, activation=\"relu\")(bert_sequence_output)\n",
    "#     print(bert_output.shape)\n",
    "    \n",
    "    timestamp_post = tf.keras.layers.Input(shape=(1,), dtype=tf.float32, name=\"timestamp_post\")\n",
    "    num_like_post = tf.keras.layers.Input(shape=(1,), dtype=tf.float32, name=\"num_like_post\")\n",
    "    num_comment_post = tf.keras.layers.Input(shape=(1,), dtype=tf.float32, name=\"num_comment_post\")\n",
    "    num_share_post = tf.keras.layers.Input(shape=(1,), dtype=tf.float32, name=\"num_share_post\")\n",
    "    \n",
    "    aulixiary_info = tf.keras.layers.Concatenate()([timestamp_post, num_like_post, num_comment_post, num_share_post])\n",
    "#     aulixiary_output  = tf.keras.layers.GaussianNoise(0.2)(aulixiary_info)\n",
    "    \n",
    "    out = tf.keras.layers.Concatenate()([bert_output, aulixiary_info]) \n",
    "#     print(out.shape)\n",
    "    out = tf.keras.layers.Dense(1, activation=\"sigmoid\")(out)\n",
    "\n",
    "    model = tf.keras.models.Model(\n",
    "        inputs=[\n",
    "            bert_input_word_ids,\n",
    "            bert_attention_mask,\n",
    "            bert_token_type_ids,  # bert input\n",
    "            timestamp_post,\n",
    "            num_like_post,\n",
    "            num_comment_post,\n",
    "            num_share_post,\n",
    "        ],\n",
    "        outputs=out,\n",
    "    )\n",
    "    model.compile(\n",
    "        tf.keras.optimizers.Adam(lr=5e-5),\n",
    "        loss=\"binary_crossentropy\",\n",
    "        metrics=[tf.keras.metrics.AUC()],\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_splits = 5\n",
    "n_epochs = 5\n",
    "\n",
    "DISPLAY=1 # USE display=1 FOR INTERACTIVE\n",
    "exp = f'phobert+auxiliary_{MAX_LEN}_len'\n",
    "\n",
    "output_dir = f'../outputs/{exp}_models'\n",
    "os.makedirs(output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scheduler(epoch):\n",
    "    return 3e-5*0.2**epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strategy = tf.distribute.MirroredStrategy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for fold, (idxT, idxV) in enumerate(kf.split(train_df)):\n",
    "for fold in sorted(train_df[\"fold\"].unique()):\n",
    "    print('*'*100)\n",
    "    print(f'FOLD: {fold+1}/{n_splits}')\n",
    "    \n",
    "    K.clear_session()\n",
    "#     with strategy.scope():\n",
    "#         model = build_model(max_len=MAX_LEN)\n",
    "    model = build_model(max_len=MAX_LEN)        \n",
    "    reduce_lr = tf.keras.callbacks.LearningRateScheduler(scheduler)\n",
    "\n",
    "    model_dir = os.path.join(output_dir, f'Fold_{fold+1}.h5')\n",
    "\n",
    "    sv = tf.keras.callbacks.ModelCheckpoint(model_dir, \n",
    "                                            monitor='val_auc', \n",
    "                                            verbose=1, \n",
    "                                            save_best_only=True,\n",
    "                                            save_weights_only=True, \n",
    "                                            mode='max', \n",
    "                                            save_freq='epoch')\n",
    "    \n",
    "    train_df_ = train_df[train_df[\"fold\"]!=fold]\n",
    "    val_df_ = train_df[train_df[\"fold\"]==fold]\n",
    "    train_dataset, valid_dataset = data_generator(train_df_, val_df_)\n",
    "    \n",
    "    n_steps = train_df_.shape[0] // BATCH_SIZE + 1\n",
    "    train_history = model.fit(\n",
    "        train_dataset,\n",
    "        steps_per_epoch=n_steps,\n",
    "        \n",
    "        callbacks=[sv, \n",
    "            reduce_lr,\n",
    "            # tb\n",
    "            ],\n",
    "        validation_data=valid_dataset,\n",
    "        epochs=n_epochs\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = regular_encode(test_df, max_len=MAX_LEN)\n",
    "y_test = np.zeros((len(test_df),1))\n",
    "test_dataset = (\n",
    "    tf.data.Dataset\n",
    "    .from_tensor_slices((X_test,y_test))\n",
    "    .batch(BATCH_SIZE)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(max_len=MAX_LEN)\n",
    "preds = []\n",
    "for i, file_name in enumerate(os.listdir(output_dir)):\n",
    "    print('_'*80)\n",
    "    \n",
    "    K.clear_session()\n",
    "    model_path = os.path.join(output_dir, file_name)\n",
    "    \n",
    "    print(f'Inferencing with model from: {model_path}')\n",
    "    model.load_weights(model_path)\n",
    "\n",
    "    pred = model.predict(test_dataset,\n",
    "                         batch_size=128,\n",
    "                         verbose=DISPLAY)\n",
    "    # print(pred[])\n",
    "    preds.append(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = np.mean(preds, axis=0)\n",
    "test_df[\"prediction\"] = preds\n",
    "test_df[\"prediction\"].to_csv(f\"{exp}.csv\", header=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
